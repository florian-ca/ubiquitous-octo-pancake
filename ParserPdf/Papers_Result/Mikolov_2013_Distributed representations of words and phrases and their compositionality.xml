<Article>
<preamble>draft</preamble>
<Titre>Distributed Representations of Words and Phrases</Titre><Titre2>and their Compositionality</Titre2>

<Auteur>Tomas Mikolov</Auteur>
<Abstract> The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.</Abstract>
<Introduction>1 Distributed representations of words in a vector space help learning algorithms to achieve better
performance in natural language processing tasks by grouping similar words. One of the earliest use
of word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. This idea
has since been applied to statistical language modeling with considerable success [1]. The follow
up work includes applications to automatic speech recognition and machine translation [14, 7], and
a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].
Recently, Mikolov et al. [8] introduced the Skip-gram model, an efficient method for learning highquality vector representations of words from large amounts of unstructured text data. Unlike most
of the previously used neural network architectures for learning word vectors, training of the Skipgram model (see Figure 1) does not involve dense matrix multiplications. This makes the training
extremely efficient: an optimized single-machine implementation can train on more than 100 billion
words in one day.
The word representations computed using neural networks are very interesting because the learned
vectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of
these patterns can be represented as linear translations. For example, the result of a vector calculation vec(“Madrid”) - vec(“Spain”) + vec(“France”) is closer to vec(“Paris”) than to any other word
vector [9, 8].
1
</Introduction>

<Bibliography>
 [1] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language
 model. The Journal of Machine Learning Research, 3:1137–1155, 2003.
 [2] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine
 learning, pages 160–167. ACM, 2008.
 [3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In ICML, 513–520, 2011.
 [4] Michael U Gutmann and Aapo Hyvärinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307–361,
 2012.
 [5] Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions of
 recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011
 IEEE International Conference on, pages 5528–5531. IEEE, 2011.
 [6] Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky. Strategies for Training
 Large Scale Neural Network Language Models. In Proc. Automatic Speech Recognition and Understanding, 2011.
 [7] Tomas Mikolov. Statistical Language Models Based on Neural Networks. PhD thesis, PhD Thesis, Brno
 University of Technology, 2012.
 [8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations
 in vector space. ICLR Workshop, 2013.
 [9] Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word
 Representations. In Proceedings of NAACL HLT, 2013.
 [10] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances in
 neural information processing systems, 21:1081–1088, 2009.
 [11] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language
 models. arXiv preprint arXiv:1206.6426, 2012.
 [12] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on artificial intelligence and statistics, pages 246–252, 2005.
 [13] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by backpropagating errors. Nature, 323(6088):533–536, 1986.
 [14] Holger Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.
 [15] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and
 natural language with recursive neural networks. In Proceedings of the 26th International Conference on
 Machine Learning (ICML), volume 2, 2011.
 [16] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic Compositionality
 Through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods
 in Natural Language Processing (EMNLP), 2012.
 [17] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for
 semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics, 2010.
 [18] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. In
 Journal of Artificial Intelligence Research, 37:141-188, 2010.
 [19] Peter D. Turney. Distributional semantics beyond words: Supervised learning of analogy and paraphrase.
 In Transactions of the Association for Computational Linguistics (TACL), 353–366, 2013.
 [20] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annotation. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume
 Volume Three, pages 2764–2770. AAAI Press, 2011.
</Bibliography>
</Article>
